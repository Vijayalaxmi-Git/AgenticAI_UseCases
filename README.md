# AgenticAI_UseCases

## Module 1: Intro to Agentic Workflows (Reflection & Iteration)

*Self-Reflection Agent: Self Reflection Pattern: â€œAn agent that evaluates and improves its own outputs before finalizing.â€  
   An agent evaluates its own output and improves it before finalizing.
*Tool-Using Agent: Tool Use Pattern: â€œAn agent that delegates deterministic tasks to tools and uses LLMs for reasoning.â€  
   An agent delegates deterministic tasks to tools and uses LLMs for reasoning.
*Planning Pattern: The agent first creates a plan (steps/subtasks) before executing actions, instead of reacting immediately.  
   The agent creates a plan before acting.

Workflow:
Planner â†’ Converts goal into steps
Executor â†’ Executes steps using tools or subâ€‘agents
Controller (optional) â†’ Tracks progress and handles failures

## Module 2: Multi-Agent Collaboration (Role-playing & Handoffs)

This module demonstrates structured collaboration between multiple LLM agents.
ðŸ”¹ module2_multi_agent.py
Two agents collaborate:
Agent A (Coder) writes code
Agent B (Reviewer) checks for bugs & security issues

ðŸ”¹ module2_crew.py
CrewAI implementation of the multi-agent workflow.

ðŸ”¹ module2_CrewOutput.txt
Sample output generated by the multi-agent system.

ðŸ”¹ OrchestrationWorkflow_3rdAgent.py
Introduces a third agent responsible for:
Orchestrating the conversation
Managing handoffs
Ensuring quality control
This demonstrates more advanced multi-agent pipelines.

## Module 3: Capabilities- Agentic Design Patterns (Tool Use & Memory)

Focuses on: Custom tools,Memory,Deterministic tool execution,Embedding-based recall
Local LLM workflows

ðŸ”¹ module3_tool_use.py
Demonstrates tool creation, registration, and deterministic execution.

ðŸ”¹ final_master_crew.py
A â€œMaster Crewâ€ orchestrator coordinating multiple agents and tools across modules.

 Agentic Design Patterns (Tool Use & Memory)
--- Created new environment ->use the script -> py -3.11 -m venv crewenv
--- For venv old environment use the script-> oldenv\Scripts\activate

## Module 4: Evaluations (Evals for Agentic AI)

In standard coding, we use "Unit Tests" (Pass/Fail).
In Agentic AI, we use Evals. Because LLMs are probabilistic (they might give different answers every time), we need to run our "Master Crew" multiple times and grade the results.
--The "Eval" Mindset
Hence implemented the three ways to evaluate your agents:
-Human Eval: You look at the output (What you've been doing).
-Deterministic Eval: Checking if the code actually runs or if the math is exactly right.
-LLM-as-a-Judge: Using a stronger model (like GPT-4 or a larger Llama) to grade your smaller model's work.
This module shows how to evaluate agentic systems reliably.

## Module 5 : Autonomous Agents (Full Autonomy & Longâ€‘Horizon Tasks)

This module demonstrates fully autonomous agents capable of:
Running without human intervention
Planning long-horizon tasks
Using tools independently
Making decisions based on memory and context

ðŸ”¹ module5_autonomous.py
A simple autonomous agent that takes a goal and attempts to complete it using reasoning loops.

ðŸ”¹ module5_final.py
A more advanced autonomous workflow combining:
Planning
Tool use
Memory
Multi-step execution
Self-correction
This script represents a near-production autonomous agent pattern.
